{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://datascience.blog.wzb.eu/2016/07/13/accurate-part-of-speech-tagging-of-german-texts-with-nltk/\n",
    "\n",
    "https://datascience.blog.wzb.eu/2016/07/13/autocorrecting-misspelled-words-in-python-using-hunspell/\n",
    "\n",
    "https://github.com/ptnplanet/NLTK-Contributions/blob/master/ClassifierBasedGermanTagger/ClassifierBasedGermanTagger.py\n",
    "\n",
    "https://github.com/WZBSocialScienceCenter/germalemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem.snowball import GermanStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import gensim\n",
    "from nltk.tag.sequential import ClassifierBasedTagger\n",
    "from nltk.corpus import ConllCorpusReader\n",
    "from sklearn.utils import shuffle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sw = '''aber           |  but\n",
    "\n",
    "alle           |  all\n",
    "allem\n",
    "allen\n",
    "aller\n",
    "alles\n",
    "\n",
    "als            |  than, as\n",
    "also           |  so\n",
    "am             |  an + dem\n",
    "an             |  at\n",
    "\n",
    "ander          |  other\n",
    "andere\n",
    "anderem\n",
    "anderen\n",
    "anderer\n",
    "anderes\n",
    "anderm\n",
    "andern\n",
    "anderr\n",
    "anders\n",
    "\n",
    "auch           |  also\n",
    "auf            |  on\n",
    "aus            |  out of\n",
    "bei            |  by\n",
    "bin            |  am\n",
    "bis            |  until\n",
    "bist           |  art\n",
    "da             |  there\n",
    "damit          |  with it\n",
    "dann           |  then\n",
    "\n",
    "der            |  the\n",
    "den\n",
    "des\n",
    "dem\n",
    "die\n",
    "das\n",
    "\n",
    "daß            |  that\n",
    "\n",
    "derselbe       |  the same\n",
    "derselben\n",
    "denselben\n",
    "desselben\n",
    "demselben\n",
    "dieselbe\n",
    "dieselben\n",
    "dasselbe\n",
    "\n",
    "dazu           |  to that\n",
    "\n",
    "dein           |  thy\n",
    "deine\n",
    "deinem\n",
    "deinen\n",
    "deiner\n",
    "deines\n",
    "\n",
    "denn           |  because\n",
    "\n",
    "derer          |  of those\n",
    "dessen         |  of him\n",
    "\n",
    "dich           |  thee\n",
    "dir            |  to thee\n",
    "du             |  thou\n",
    "\n",
    "dies           |  this\n",
    "diese\n",
    "diesem\n",
    "diesen\n",
    "dieser\n",
    "dieses\n",
    "\n",
    "\n",
    "doch           |  (several meanings)\n",
    "dort           |  (over) there\n",
    "\n",
    "\n",
    "durch          |  through\n",
    "\n",
    "ein            |  a\n",
    "eine\n",
    "einem\n",
    "einen\n",
    "einer\n",
    "eines\n",
    "\n",
    "einig          |  some\n",
    "einige\n",
    "einigem\n",
    "einigen\n",
    "einiger\n",
    "einiges\n",
    "\n",
    "einmal         |  once\n",
    "\n",
    "er             |  he\n",
    "ihn            |  him\n",
    "ihm            |  to him\n",
    "\n",
    "es             |  it\n",
    "etwas          |  something\n",
    "\n",
    "euer           |  your\n",
    "eure\n",
    "eurem\n",
    "euren\n",
    "eurer\n",
    "eures\n",
    "\n",
    "für            |  for\n",
    "gegen          |  towards\n",
    "gewesen        |  p.p. of sein\n",
    "hab            |  have\n",
    "habe           |  have\n",
    "haben          |  have\n",
    "hat            |  has\n",
    "hatte          |  had\n",
    "hatten         |  had\n",
    "hier           |  here\n",
    "hin            |  there\n",
    "hinter         |  behind\n",
    "\n",
    "ich            |  I\n",
    "mich           |  me\n",
    "mir            |  to me\n",
    "\n",
    "\n",
    "ihr            |  you, to her\n",
    "ihre\n",
    "ihrem\n",
    "ihren\n",
    "ihrer\n",
    "ihres\n",
    "euch           |  to you\n",
    "\n",
    "im             |  in + dem\n",
    "in             |  in\n",
    "indem          |  while\n",
    "ins            |  in + das\n",
    "ist            |  is\n",
    "\n",
    "jede           |  each, every\n",
    "jedem\n",
    "jeden\n",
    "jeder\n",
    "jedes\n",
    "\n",
    "jene           |  that\n",
    "jenem\n",
    "jenen\n",
    "jener\n",
    "jenes\n",
    "\n",
    "jetzt          |  now\n",
    "kann           |  can\n",
    "\n",
    "kein           |  no\n",
    "keine\n",
    "keinem\n",
    "keinen\n",
    "keiner\n",
    "keines\n",
    "\n",
    "können         |  can\n",
    "könnte         |  could\n",
    "machen         |  do\n",
    "man            |  one\n",
    "\n",
    "manche         |  some, many a\n",
    "manchem\n",
    "manchen\n",
    "mancher\n",
    "manches\n",
    "\n",
    "mein           |  my\n",
    "meine\n",
    "meinem\n",
    "meinen\n",
    "meiner\n",
    "meines\n",
    "\n",
    "mit            |  with\n",
    "muss           |  must\n",
    "musste         |  had to\n",
    "nach           |  to(wards)\n",
    "nicht          |  not\n",
    "nichts         |  nothing\n",
    "noch           |  still, yet\n",
    "nun            |  now\n",
    "nur            |  only\n",
    "ob             |  whether\n",
    "oder           |  or\n",
    "ohne           |  without\n",
    "sehr           |  very\n",
    "\n",
    "sein           |  his\n",
    "seine\n",
    "seinem\n",
    "seinen\n",
    "seiner\n",
    "seines\n",
    "\n",
    "selbst         |  self\n",
    "sich           |  herself\n",
    "\n",
    "sie            |  they, she\n",
    "ihnen          |  to them\n",
    "\n",
    "sind           |  are\n",
    "so             |  so\n",
    "\n",
    "solche         |  such\n",
    "solchem\n",
    "solchen\n",
    "solcher\n",
    "solches\n",
    "\n",
    "soll           |  shall\n",
    "sollte         |  should\n",
    "sondern        |  but\n",
    "sonst          |  else\n",
    "über           |  over\n",
    "um             |  about, around\n",
    "und            |  and\n",
    "\n",
    "uns            |  us\n",
    "unse\n",
    "unsem\n",
    "unsen\n",
    "unser\n",
    "unses\n",
    "\n",
    "unter          |  under\n",
    "viel           |  much\n",
    "vom            |  von + dem\n",
    "von            |  from\n",
    "vor            |  before\n",
    "während        |  while\n",
    "war            |  was\n",
    "waren          |  were\n",
    "warst          |  wast\n",
    "was            |  what\n",
    "weg            |  away, off\n",
    "weil           |  because\n",
    "weiter         |  further\n",
    "\n",
    "welche         |  which\n",
    "welchem\n",
    "welchen\n",
    "welcher\n",
    "welches\n",
    "\n",
    "wenn           |  when\n",
    "werde          |  will\n",
    "werden         |  will\n",
    "wie            |  how\n",
    "wieder         |  again\n",
    "will           |  want\n",
    "wir            |  we\n",
    "wird           |  will\n",
    "wirst          |  willst\n",
    "wo             |  where\n",
    "wollen         |  want\n",
    "wollte         |  wanted\n",
    "würde          |  would\n",
    "würden         |  would\n",
    "zu             |  to\n",
    "zum            |  zu + dem\n",
    "zur            |  zu + der\n",
    "zwar           |  indeed\n",
    "zwischen       |  between\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = [\n",
    "    'augsburger_allgemeine',\n",
    "    'badische_zeitung',\n",
    "    'berliner_zeitung',\n",
    "    'frankfurter_allgemeine_zeitung',\n",
    "    'koelner_stadt_anzeiger',\n",
    "    'leipziger_volkszeitung',\n",
    "    'mitteldeutsche_zeitung',\n",
    "    'rheinische_post',\n",
    "    'saechsische_zeitung',\n",
    "    'stuttgarter_zeitung',\n",
    "    'sueddeutsche_zeitung',\n",
    "    'weser_kurier',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = []\n",
    "dates = [\"oct31\", 'nov1', 'nov2', 'nov3', 'nov4']\n",
    "for name in names:\n",
    "    for date in dates:\n",
    "        filename = \"data/\" + name + \"/\" + name + \"_\" + date + \".json\"\n",
    "        filenames.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some sites have two headlines -- combine them.\n",
    "def combine_headlines(df, filename):\n",
    "    if 'article_headline_strong' in df.columns:\n",
    "        df['article_headline'] = df['article_headline_strong'] + \" \" + df['article_headline_normal']\n",
    "        df.drop(['article_headline_strong', 'article_headline_normal'], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prep_article(df):\n",
    "    df.article_text = df.article_text.apply(lambda x: ' '.join(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success:  data/augsburger_allgemeine/augsburger_allgemeine_oct31.json\n",
      "Success:  data/augsburger_allgemeine/augsburger_allgemeine_nov1.json\n",
      "Success:  data/augsburger_allgemeine/augsburger_allgemeine_nov2.json\n",
      "Success:  data/augsburger_allgemeine/augsburger_allgemeine_nov3.json\n",
      "Success:  data/augsburger_allgemeine/augsburger_allgemeine_nov4.json\n",
      "Success:  data/badische_zeitung/badische_zeitung_oct31.json\n",
      "Success:  data/badische_zeitung/badische_zeitung_nov1.json\n",
      "Success:  data/badische_zeitung/badische_zeitung_nov2.json\n",
      "Success:  data/badische_zeitung/badische_zeitung_nov3.json\n",
      "Success:  data/badische_zeitung/badische_zeitung_nov4.json\n",
      "Success:  data/berliner_zeitung/berliner_zeitung_oct31.json\n",
      "Success:  data/berliner_zeitung/berliner_zeitung_nov1.json\n",
      "Success:  data/berliner_zeitung/berliner_zeitung_nov2.json\n",
      "Success:  data/berliner_zeitung/berliner_zeitung_nov3.json\n",
      "Success:  data/berliner_zeitung/berliner_zeitung_nov4.json\n",
      "Success:  data/frankfurter_allgemeine_zeitung/frankfurter_allgemeine_zeitung_oct31.json\n",
      "Success:  data/frankfurter_allgemeine_zeitung/frankfurter_allgemeine_zeitung_nov1.json\n",
      "Success:  data/frankfurter_allgemeine_zeitung/frankfurter_allgemeine_zeitung_nov2.json\n",
      "Success:  data/frankfurter_allgemeine_zeitung/frankfurter_allgemeine_zeitung_nov3.json\n",
      "Success:  data/frankfurter_allgemeine_zeitung/frankfurter_allgemeine_zeitung_nov4.json\n",
      "Success:  data/koelner_stadt_anzeiger/koelner_stadt_anzeiger_oct31.json\n",
      "Success:  data/koelner_stadt_anzeiger/koelner_stadt_anzeiger_nov1.json\n",
      "Success:  data/koelner_stadt_anzeiger/koelner_stadt_anzeiger_nov2.json\n",
      "Success:  data/koelner_stadt_anzeiger/koelner_stadt_anzeiger_nov3.json\n",
      "Success:  data/koelner_stadt_anzeiger/koelner_stadt_anzeiger_nov4.json\n",
      "Success:  data/leipziger_volkszeitung/leipziger_volkszeitung_oct31.json\n",
      "Success:  data/leipziger_volkszeitung/leipziger_volkszeitung_nov1.json\n",
      "Success:  data/leipziger_volkszeitung/leipziger_volkszeitung_nov2.json\n",
      "Success:  data/leipziger_volkszeitung/leipziger_volkszeitung_nov3.json\n",
      "Success:  data/leipziger_volkszeitung/leipziger_volkszeitung_nov4.json\n",
      "Success:  data/mitteldeutsche_zeitung/mitteldeutsche_zeitung_oct31.json\n",
      "Success:  data/mitteldeutsche_zeitung/mitteldeutsche_zeitung_nov1.json\n",
      "Success:  data/mitteldeutsche_zeitung/mitteldeutsche_zeitung_nov2.json\n",
      "Success:  data/mitteldeutsche_zeitung/mitteldeutsche_zeitung_nov3.json\n",
      "Success:  data/mitteldeutsche_zeitung/mitteldeutsche_zeitung_nov4.json\n",
      "Success:  data/rheinische_post/rheinische_post_oct31.json\n",
      "Success:  data/rheinische_post/rheinische_post_nov1.json\n",
      "Success:  data/rheinische_post/rheinische_post_nov2.json\n",
      "Success:  data/rheinische_post/rheinische_post_nov3.json\n",
      "Success:  data/rheinische_post/rheinische_post_nov4.json\n",
      "Success:  data/saechsische_zeitung/saechsische_zeitung_oct31.json\n",
      "Success:  data/saechsische_zeitung/saechsische_zeitung_nov1.json\n",
      "Success:  data/saechsische_zeitung/saechsische_zeitung_nov2.json\n",
      "Success:  data/saechsische_zeitung/saechsische_zeitung_nov3.json\n",
      "Success:  data/saechsische_zeitung/saechsische_zeitung_nov4.json\n",
      "Success:  data/stuttgarter_zeitung/stuttgarter_zeitung_oct31.json\n",
      "Success:  data/stuttgarter_zeitung/stuttgarter_zeitung_nov1.json\n",
      "Success:  data/stuttgarter_zeitung/stuttgarter_zeitung_nov2.json\n",
      "Success:  data/stuttgarter_zeitung/stuttgarter_zeitung_nov3.json\n",
      "Success:  data/stuttgarter_zeitung/stuttgarter_zeitung_nov4.json\n",
      "\n",
      "ERROR ::::  data/sueddeutsche_zeitung/sueddeutsche_zeitung_oct31.json \n",
      "\n",
      "Success:  data/sueddeutsche_zeitung/sueddeutsche_zeitung_nov1.json\n",
      "Success:  data/sueddeutsche_zeitung/sueddeutsche_zeitung_nov2.json\n",
      "Success:  data/sueddeutsche_zeitung/sueddeutsche_zeitung_nov3.json\n",
      "Success:  data/sueddeutsche_zeitung/sueddeutsche_zeitung_nov4.json\n",
      "\n",
      "ERROR ::::  data/weser_kurier/weser_kurier_oct31.json \n",
      "\n",
      "Success:  data/weser_kurier/weser_kurier_nov1.json\n",
      "Success:  data/weser_kurier/weser_kurier_nov2.json\n",
      "Success:  data/weser_kurier/weser_kurier_nov3.json\n",
      "Success:  data/weser_kurier/weser_kurier_nov4.json\n"
     ]
    }
   ],
   "source": [
    "# Print an error message if there is a problem reading in a filename.\n",
    "\n",
    "dfs = []\n",
    "for filename in filenames:\n",
    "    try:\n",
    "        with open(filename, 'r') as f_obj:\n",
    "            df = pd.read_json(f_obj)\n",
    "    except FileNotFoundError:\n",
    "        print(\"\\nERROR :::: \", filename, \"\\n\")\n",
    "    else:\n",
    "        df['source'] = filename.split('/')[1]\n",
    "        combine_headlines(df, filename)\n",
    "        prep_article(df)\n",
    "        print(\"Success: \", filename)\n",
    "        dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This command creates the actual DataFrame.\n",
    "\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2543410"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many total words are in the corpus?\n",
    "\n",
    "a = df.dropna().article_text\n",
    "a = np.array(a)\n",
    "big_text = []\n",
    "for i in a:\n",
    "    i = re.sub(r'[^A-Za-z ]*', '', i)\n",
    "    big_text.append(i)\n",
    "corpus = \" \".join(big_text)\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put all article text into a list of big strings.\n",
    "# May need to go back and df.dropna()\n",
    "# Plus text cleaning.  get rid of punctuation? etc.\n",
    "\n",
    "\n",
    "corpus = []\n",
    "gsm = GermanStemmer()\n",
    "for row in df.iterrows():\n",
    "    article_text = row[1][2]\n",
    "    article_text = article_text.lower()\n",
    "    article_text = re.sub(r'[^A-Za-zäüöß ]*', '', article_text)\n",
    "    stemmed = gsm.stem(article_text)\n",
    "    tokenized = word_tokenize(stemmed)\n",
    "    lemmatized = lemmatize_german(tokenized)\n",
    "    corpus.append(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(corpus, size=100, window=10, min_count=3, workers=2,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('einst', <gensim.models.keyedvectors.Vocab at 0x121075208>),\n",
       " ('starten', <gensim.models.keyedvectors.Vocab at 0x121075630>),\n",
       " ('auf', <gensim.models.keyedvectors.Vocab at 0x121075710>),\n",
       " ('der', <gensim.models.keyedvectors.Vocab at 0x121075ac8>),\n",
       " ('gelande', <gensim.models.keyedvectors.Vocab at 0x12129e160>),\n",
       " ('bundeswehr', <gensim.models.keyedvectors.Vocab at 0x12129e080>),\n",
       " ('nachdem', <gensim.models.keyedvectors.Vocab at 0x12129e978>),\n",
       " ('letzter', <gensim.models.keyedvectors.Vocab at 0x12129e198>),\n",
       " ('aus', <gensim.models.keyedvectors.Vocab at 0x12129ecf8>),\n",
       " ('langst', <gensim.models.keyedvectors.Vocab at 0x12129eef0>),\n",
       " ('abziehen', <gensim.models.keyedvectors.Vocab at 0x12129e9e8>),\n",
       " ('sein', <gensim.models.keyedvectors.Vocab at 0x12129ee80>),\n",
       " ('sollen', <gensim.models.keyedvectors.Vocab at 0x12129ef28>),\n",
       " ('dort', <gensim.models.keyedvectors.Vocab at 0x12129edd8>),\n",
       " ('nun', <gensim.models.keyedvectors.Vocab at 0x12129e6a0>),\n",
       " ('gedeihen', <gensim.models.keyedvectors.Vocab at 0x12129ee10>),\n",
       " ('in', <gensim.models.keyedvectors.Vocab at 0x12129e0b8>),\n",
       " ('ein', <gensim.models.keyedvectors.Vocab at 0x12129e6d8>),\n",
       " ('ehemalig', <gensim.models.keyedvectors.Vocab at 0x12129e550>),\n",
       " ('einer', <gensim.models.keyedvectors.Vocab at 0x12129ed68>)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.wv.vocab.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('differenzen', 0.9250619411468506),\n",
       " ('sondierer', 0.92262202501297),\n",
       " ('fortschritte', 0.9128716588020325),\n",
       " ('problemen', 0.8996192812919617),\n",
       " ('bekenntnis', 0.8994232416152954),\n",
       " ('einfuhren', 0.8973312973976135),\n",
       " ('gezielt', 0.8950703144073486),\n",
       " ('jamaikasondierungen', 0.8910605907440186),\n",
       " ('klimaschutz', 0.8854465484619141),\n",
       " ('bundeln', 0.8846082091331482)]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YES\n",
    "\n",
    "model.most_similar('ostdeutsch', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sechzig', 0.969581663608551),\n",
       " ('altenpflege', 0.9694417715072632),\n",
       " ('skandalen', 0.9664402008056641),\n",
       " ('schlagzeile', 0.9612240791320801),\n",
       " ('ruff', 0.9607419371604919),\n",
       " ('grossartiges', 0.9607359170913696),\n",
       " ('arbeitsbedingungen', 0.9607052206993103),\n",
       " ('berufsschule', 0.9606802463531494),\n",
       " ('ddrzeiten', 0.9593487977981567),\n",
       " ('bruche', 0.9592036008834839)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YES\n",
    "\n",
    "model.most_similar('westdeutsch', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing tools thanks to the internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ClassifierBasedGermanTagger(ClassifierBasedTagger):\n",
    "    \"\"\"A classifier based German part-of-speech tagger. It has an accuracy of\n",
    "    96.09% after being trained on 90% of the German TIGER corpus. The tagger\n",
    "    extends the NLTK ClassifierBasedTagger and implements a slightly modified\n",
    "    feature detector.\n",
    "    \"\"\"\n",
    "\n",
    "    def feature_detector(self, tokens, index, history):\n",
    "        \"\"\"Implementing a slightly modified feature detector.\n",
    "        @param tokens: The tokens from the sentence to tag.\n",
    "        @param index: The current token index to tag.\n",
    "        @param history: The previous tagged tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        word = tokens[index]\n",
    "        if index == 0: # At the beginning of the sentence\n",
    "            prevword = prevprevword = None\n",
    "            prevtag = prevprevtag = None\n",
    "            #word = word.lower() # Lowercase at the beginning of sentence\n",
    "        elif index == 1:\n",
    "            prevword = tokens[index-1] # Note: no lowercase\n",
    "            prevprevword = None\n",
    "            prevtag = history[index-1]\n",
    "            prevprevtag = None\n",
    "        else:\n",
    "            prevword = tokens[index-1]\n",
    "            prevprevword = tokens[index-2]\n",
    "            prevtag = history[index-1]\n",
    "            prevprevtag = history[index-2]\n",
    "\n",
    "        if re.match('[0-9]+([\\.,][0-9]*)?|[0-9]*[\\.,][0-9]+$', word):\n",
    "            # Included \",\" as decimal point\n",
    "            shape = 'number'\n",
    "        elif re.compile('\\W+$', re.UNICODE).match(word):\n",
    "            # Included unicode flag\n",
    "            shape = 'punct'\n",
    "        elif re.match('([A-ZÄÖÜ]+[a-zäöüß]*-?)+$', word):\n",
    "            # Included dash for dashed words and umlauts\n",
    "            shape = 'upcase'\n",
    "        elif re.match('[a-zäöüß]+', word):\n",
    "            # Included umlauts\n",
    "            shape = 'downcase'\n",
    "        elif re.compile(\"\\w+\", re.UNICODE).match(word):\n",
    "            # Included unicode flag\n",
    "            shape = 'mixedcase'\n",
    "        else:\n",
    "            shape = 'other'\n",
    "\n",
    "        features = {\n",
    "            'prevtag': prevtag,\n",
    "            'prevprevtag': prevprevtag,\n",
    "            'word': word,\n",
    "            'word.lower': word.lower(),\n",
    "            'suffix3': word.lower()[-3:],\n",
    "            #'suffix2': word.lower()[-2:],\n",
    "            #'suffix1': word.lower()[-1:],\n",
    "            'preffix1': word[:1], # included\n",
    "            'prevprevword': prevprevword,\n",
    "            'prevword': prevword,\n",
    "            'prevtag+word': '%s+%s' % (prevtag, word),\n",
    "            'prevprevtag+word': '%s+%s' % (prevprevtag, word),\n",
    "            'prevword+word': '%s+%s' % (prevword, word),\n",
    "            'shape': shape\n",
    "            }\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp = ConllCorpusReader('.', 'tiger_release_aug07.corrected.16012013.conll09',\n",
    "                                     ['ignore', 'words', 'ignore', 'ignore', 'pos'],\n",
    "                                     encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_sents = corp.tagged_sents()\n",
    "shuffle(tagged_sents)\n",
    "split_perc = 0.1\n",
    "split_size = int(len(tagged_sents) * split_perc)\n",
    "train_sents, test_sents = tagged_sents[split_size:], tagged_sents[:split_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagger = ClassifierBasedGermanTagger(train=train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_lemmata_from_tiger_corpus(tiger_corpus_file, valid_cols_n=15, col_words=1, col_lemmata=2):\n",
    "    lemmata_mapping = {}\n",
    "\n",
    "    with open(tiger_corpus_file) as f:\n",
    "        for line in f:\n",
    "            parts = line.split()\n",
    "            if len(parts) == valid_cols_n:\n",
    "                w, lemma = parts[col_words], parts[col_lemmata]\n",
    "                if w != lemma and w not in lemmata_mapping and not lemma.startswith('--'):\n",
    "                    lemmata_mapping[w] = lemma\n",
    "\n",
    "    return lemmata_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a dictionary that maps words to their lemma.\n",
    "\n",
    "lemmata_mapping = read_lemmata_from_tiger_corpus('tiger_release_aug07.corrected.16012013.conll09')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatize_german(corpus):\n",
    "    lemmata = []\n",
    "    for w in corpus:\n",
    "        w_lemma = lemmata_mapping.get(w, None)\n",
    "        if w_lemma:\n",
    "            lemmata.append(w_lemma)\n",
    "        else:\n",
    "            lemmata.append(w)\n",
    "    return lemmata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_article_string = df.iloc[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_article_list = word_tokenize(test_article_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Einst',\n",
       " 'starteten',\n",
       " 'auf',\n",
       " 'dem',\n",
       " 'Gelände',\n",
       " 'die',\n",
       " 'Kampfflugzeuge',\n",
       " 'der',\n",
       " 'Bundeswehr',\n",
       " '.']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# string --> word_tokenize --> list of strings\n",
    "test_article_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['einst',\n",
       " 'starten',\n",
       " 'auf',\n",
       " 'der',\n",
       " 'Gelände',\n",
       " 'der',\n",
       " 'Kampfflugzeug',\n",
       " 'der',\n",
       " 'Bundeswehr',\n",
       " '-',\n",
       " 'nachdem',\n",
       " 'der',\n",
       " 'letzter',\n",
       " '``',\n",
       " 'Tornado',\n",
       " \"''\",\n",
       " 'aus',\n",
       " 'Memmingerberg',\n",
       " 'lang',\n",
       " 'abziehen',\n",
       " 'sein',\n",
       " ',',\n",
       " 'sollen',\n",
       " 'dort',\n",
       " 'nun',\n",
       " 'Hanfpflanzen',\n",
       " 'gedeihen',\n",
       " '-',\n",
       " 'in',\n",
       " 'ein',\n",
       " 'Atomschutzbunker',\n",
       " 'der',\n",
       " 'ehemalig',\n",
       " 'Fliegerhorstes',\n",
       " 'einer',\n",
       " 'paar',\n",
       " 'Kilometer',\n",
       " 'von',\n",
       " 'Memmingen',\n",
       " 'entfernen',\n",
       " 'wollen',\n",
       " 'Wissenschaftler',\n",
       " 'künftig',\n",
       " 'an',\n",
       " 'Cannabispflanze',\n",
       " 'forschen',\n",
       " '-',\n",
       " 'der',\n",
       " 'Projekt',\n",
       " 'werden',\n",
       " 'von',\n",
       " 'ein',\n",
       " 'Unternehmer',\n",
       " 'aus',\n",
       " 'Schwabe',\n",
       " 'und',\n",
       " 'der',\n",
       " 'technisch',\n",
       " 'Universität',\n",
       " 'München',\n",
       " 'vorantreiben',\n",
       " '-',\n",
       " 'noch',\n",
       " 'stehen',\n",
       " 'allerdings',\n",
       " 'der',\n",
       " 'Genehmigung',\n",
       " 'der',\n",
       " 'Bundesinstitut',\n",
       " 'für',\n",
       " 'Arzneimittel',\n",
       " 'und',\n",
       " 'Medizinprodukt',\n",
       " 'aus',\n",
       " '-',\n",
       " 'sollen',\n",
       " 'der',\n",
       " 'Erlaubnis',\n",
       " 'bis',\n",
       " 'Ende',\n",
       " 'der',\n",
       " 'Jahr',\n",
       " 'vorliegen',\n",
       " ',',\n",
       " 'können',\n",
       " 'der',\n",
       " 'Anbau',\n",
       " 'der',\n",
       " 'Cannabispflanze',\n",
       " 'in',\n",
       " 'Frühjahr',\n",
       " '2018',\n",
       " 'starten',\n",
       " ',',\n",
       " 'sagen',\n",
       " 'Professor',\n",
       " 'Wolfgang',\n",
       " 'Eisenreich',\n",
       " 'von',\n",
       " 'der',\n",
       " 'Fakultät',\n",
       " 'für',\n",
       " 'Chemie',\n",
       " 'der',\n",
       " 'TU',\n",
       " '-',\n",
       " '``',\n",
       " 'wir',\n",
       " 'stehen',\n",
       " 'in',\n",
       " 'der',\n",
       " 'Startloch',\n",
       " '-',\n",
       " \"''\",\n",
       " 'der',\n",
       " 'Forschungsprojekt',\n",
       " 'werden',\n",
       " 'dann',\n",
       " 'mindestens',\n",
       " 'drei',\n",
       " 'bis',\n",
       " 'vier',\n",
       " 'Jahr',\n",
       " 'laufen',\n",
       " '-',\n",
       " 'für',\n",
       " 'der',\n",
       " 'Erforschung',\n",
       " 'sollen',\n",
       " 'mehrere',\n",
       " 'Doktorandenstellen',\n",
       " 'schaffen',\n",
       " 'werden',\n",
       " '-',\n",
       " 'Eisenreich',\n",
       " 'möchten',\n",
       " 'der',\n",
       " 'Anbau',\n",
       " 'und',\n",
       " 'der',\n",
       " 'Inhaltsstoffe',\n",
       " 'von',\n",
       " 'mindestens',\n",
       " '80',\n",
       " 'verschieden',\n",
       " 'Zuchtlinien',\n",
       " 'untersuchen',\n",
       " '-',\n",
       " '``',\n",
       " 'es',\n",
       " 'geben',\n",
       " 'relativ',\n",
       " 'weniger',\n",
       " 'wissenschaftlich',\n",
       " 'Studie',\n",
       " 'zu',\n",
       " 'Cannabi',\n",
       " \"''\",\n",
       " ',',\n",
       " 'erklären',\n",
       " 'er',\n",
       " '-',\n",
       " 'nach',\n",
       " 'der',\n",
       " 'Anbau',\n",
       " 'in',\n",
       " 'der',\n",
       " 'Atombunker',\n",
       " 'werden',\n",
       " 'der',\n",
       " 'Extrakte',\n",
       " 'der',\n",
       " 'Cannabispflanze',\n",
       " 'zu',\n",
       " 'TU-Campus',\n",
       " 'in',\n",
       " 'Garching',\n",
       " 'bei',\n",
       " 'München',\n",
       " 'zu',\n",
       " 'Laboranalyse',\n",
       " 'bringen',\n",
       " '-',\n",
       " 'der',\n",
       " 'Allgäuer',\n",
       " 'Gesellschaft',\n",
       " 'Bunker',\n",
       " 'Pflanzenextrakte',\n",
       " 'wollen',\n",
       " 'der',\n",
       " 'früh',\n",
       " 'Atombunker',\n",
       " 'für',\n",
       " 'der',\n",
       " 'Anbau',\n",
       " 'nutzen',\n",
       " ',',\n",
       " 'weil',\n",
       " 'der',\n",
       " 'Militäranlage',\n",
       " 'aller',\n",
       " 'Sicherheitsvoraussetzungen',\n",
       " 'erfüllen',\n",
       " '-',\n",
       " 'Geschäftsführer',\n",
       " 'Christoph',\n",
       " 'Rossner',\n",
       " 'setzen',\n",
       " 'sich',\n",
       " 'nach',\n",
       " 'eigen',\n",
       " 'Angabe',\n",
       " 'seit',\n",
       " '17',\n",
       " 'Jahr',\n",
       " 'für',\n",
       " 'der',\n",
       " 'Zulassung',\n",
       " 'und',\n",
       " 'Erforschung',\n",
       " 'von',\n",
       " 'Cannabi',\n",
       " 'als',\n",
       " 'Arznei',\n",
       " 'einer',\n",
       " '-',\n",
       " 'er',\n",
       " 'haben',\n",
       " 'mehrere',\n",
       " 'Geldgeber',\n",
       " 'in',\n",
       " 'Hintergrund',\n",
       " ',',\n",
       " 'sagen',\n",
       " 'der',\n",
       " '47-Jährige',\n",
       " '-',\n",
       " 'Name',\n",
       " 'nennen',\n",
       " 'er',\n",
       " 'allerdings',\n",
       " 'nicht',\n",
       " '-',\n",
       " 'um',\n",
       " 'der',\n",
       " 'bislang',\n",
       " 'schwierig',\n",
       " 'Versorgung',\n",
       " 'schwer',\n",
       " 'krank',\n",
       " 'Patient',\n",
       " 'sicherstellen',\n",
       " ',',\n",
       " 'sollen',\n",
       " 'auch',\n",
       " 'in',\n",
       " 'Cannabi',\n",
       " 'zu',\n",
       " 'medizinisch',\n",
       " 'Zweck',\n",
       " 'anbauen',\n",
       " 'werden',\n",
       " '-',\n",
       " 'ein',\n",
       " 'in',\n",
       " 'März',\n",
       " '2017',\n",
       " 'in',\n",
       " 'Kraft',\n",
       " 'getretenes',\n",
       " 'Gesetz',\n",
       " 'haben',\n",
       " 'der',\n",
       " 'Möglichkeit',\n",
       " 'der',\n",
       " 'therapeutisch',\n",
       " 'Nutzung',\n",
       " 'der',\n",
       " 'Droge',\n",
       " 'erweitern',\n",
       " '-',\n",
       " 'bei',\n",
       " 'Bundesinstitut',\n",
       " 'werden',\n",
       " 'ein',\n",
       " 'Cannabisagentur',\n",
       " 'einrichten',\n",
       " ',',\n",
       " 'um',\n",
       " 'der',\n",
       " 'Handel',\n",
       " 'zu',\n",
       " 'medizinisch',\n",
       " 'Zweck',\n",
       " 'staatlich',\n",
       " 'zu',\n",
       " 'überwachen',\n",
       " '-',\n",
       " 'Cannabi',\n",
       " 'können',\n",
       " 'beispielsweise',\n",
       " 'bei',\n",
       " 'Schmerzpatienten',\n",
       " 'und',\n",
       " 'kranke',\n",
       " 'mit',\n",
       " 'Multipler',\n",
       " 'Sklerose',\n",
       " 'von',\n",
       " 'der',\n",
       " 'Arzt',\n",
       " 'verschreiben',\n",
       " 'werden',\n",
       " '-',\n",
       " 'für',\n",
       " 'der',\n",
       " 'Zeitraum',\n",
       " '2019',\n",
       " 'bis',\n",
       " '2022',\n",
       " 'werden',\n",
       " 'der',\n",
       " 'Lieferung',\n",
       " 'von',\n",
       " 'Cannabi',\n",
       " 'in',\n",
       " 'Umfang',\n",
       " 'von',\n",
       " '6600',\n",
       " 'Kilogramm',\n",
       " 'von',\n",
       " 'der',\n",
       " 'Behörde',\n",
       " 'ausschreiben',\n",
       " '-',\n",
       " 'mehr',\n",
       " 'als',\n",
       " '100',\n",
       " 'Interessent',\n",
       " 'haben',\n",
       " 'sich',\n",
       " 'an',\n",
       " 'der',\n",
       " 'Verfahren',\n",
       " 'beteiligen',\n",
       " ',',\n",
       " 'wie',\n",
       " 'aus',\n",
       " 'ein',\n",
       " 'Antwort',\n",
       " 'der',\n",
       " 'Bundesregierung',\n",
       " 'auf',\n",
       " 'ein',\n",
       " 'Anfrage',\n",
       " 'der',\n",
       " 'linke',\n",
       " 'hervorgehen',\n",
       " '-',\n",
       " 'es',\n",
       " 'handeln',\n",
       " 'sich',\n",
       " 'zumeist',\n",
       " 'um',\n",
       " 'Unternehmen',\n",
       " 'aus',\n",
       " 'Deutschland',\n",
       " ',',\n",
       " 'aber',\n",
       " 'auch',\n",
       " 'um',\n",
       " 'Interessent',\n",
       " 'aus',\n",
       " 'Israel',\n",
       " ',',\n",
       " 'Kanada',\n",
       " ',',\n",
       " 'der',\n",
       " 'Nierderlande',\n",
       " ',',\n",
       " 'der',\n",
       " 'Schweiz',\n",
       " ',',\n",
       " 'Uruguay',\n",
       " ',',\n",
       " 'der',\n",
       " 'USA',\n",
       " 'und',\n",
       " 'Zypern',\n",
       " '-',\n",
       " 'der',\n",
       " 'Ausschreibungsverfahren',\n",
       " 'sein',\n",
       " 'nach',\n",
       " 'Angabe',\n",
       " 'ein',\n",
       " 'Sprecherin',\n",
       " 'der',\n",
       " 'Bundesinstitut',\n",
       " 'noch',\n",
       " 'nicht',\n",
       " 'abschließen',\n",
       " '-',\n",
       " 'der',\n",
       " 'Atombunker',\n",
       " 'sein',\n",
       " 'einer',\n",
       " 'Relikt',\n",
       " 'aus',\n",
       " 'der',\n",
       " 'Zeit',\n",
       " 'der',\n",
       " 'kalt',\n",
       " 'Krieg',\n",
       " '-',\n",
       " 'auf',\n",
       " 'der',\n",
       " 'Fliegerhorst',\n",
       " 'sein',\n",
       " 'einst',\n",
       " 'der',\n",
       " 'Jagdbombergeschwader',\n",
       " '34',\n",
       " '``',\n",
       " 'Allgäu',\n",
       " \"''\",\n",
       " 'der',\n",
       " 'stationieren',\n",
       " '-',\n",
       " 'nach',\n",
       " 'mehrere',\n",
       " 'als',\n",
       " '40',\n",
       " 'Jahr',\n",
       " 'werden',\n",
       " 'der',\n",
       " 'Einheit',\n",
       " '2003',\n",
       " 'auflösen',\n",
       " 'und',\n",
       " 'der',\n",
       " 'Militärflugplatz',\n",
       " 'schließen',\n",
       " '-',\n",
       " 'der',\n",
       " 'Gelände',\n",
       " 'werden',\n",
       " 'nun',\n",
       " 'von',\n",
       " 'Allgäu',\n",
       " 'Airport',\n",
       " 'nutzen',\n",
       " '-',\n",
       " 'Unternehmer',\n",
       " 'Rossner',\n",
       " 'können',\n",
       " 'sich',\n",
       " 'vorstellen',\n",
       " ',',\n",
       " 'daß',\n",
       " 'der',\n",
       " 'Bunker',\n",
       " 'auch',\n",
       " 'nach',\n",
       " 'der',\n",
       " 'Forschungsprojekt',\n",
       " 'für',\n",
       " 'der',\n",
       " 'Cannabisanbau',\n",
       " 'in',\n",
       " 'Rahmen',\n",
       " 'der',\n",
       " 'staatlich',\n",
       " 'Beschaffungsprojekts',\n",
       " 'weit',\n",
       " 'verwenden',\n",
       " 'werden',\n",
       " '-']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_german(test_article_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
